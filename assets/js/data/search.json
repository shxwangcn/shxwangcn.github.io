[ { "title": "总结Linux中常见的命令行工具", "url": "/posts/linux-commands/", "categories": "linux, commands", "tags": "linux,find,grep,sed,awk,netstat,", "date": "2022-04-21 09:20:12 +0800", "snippet": "本篇文章总结下日常开发中比较常用的命令行工具。 持续更新中…包括以下工具： 文本文件类： 文件查找 find 文件搜索 grep 文件替换 sed 文件分析 awk 网络分析类： netstat nc nslookup &amp; dig tcpdump 文件传输&...", "content": "本篇文章总结下日常开发中比较常用的命令行工具。 持续更新中…包括以下工具： 文本文件类： 文件查找 find 文件搜索 grep 文件替换 sed 文件分析 awk 网络分析类： netstat nc nslookup &amp; dig tcpdump 文件传输&amp;下载： wget curl rsync 其它： xargs 本篇文章会不定期更新，以便新增其它的命令。此外，有些命令因为过于复杂，所以会使用单独的文章来记录： 使用 iptables 管理防火墙对于每个命令，这里只会记录我日常用到的功能，以便随时查看。对于每个命令的完整性使用指南，还是 man xxx 吧。1 文本文件操作工具find功能：支持通过文件的类型、名称、时间等过滤器来查找文件。支持正则表达式条件。使用语法：find [-H | -L | -P] [-EXdsx] [-f path] path ... [expression]包括三部分： options 控制find的行为，其中，-H | -L | -P 控制对软连接的处理逻辑；-Exdsx等各有用处，具体参考man手册； Primaries 控制查找的条件，支持按文件类型、时间、权限等等信息来查找； path 查找的目录下面给出几个示例：示例1 找出当前目录下所有的有可执行权限的普通文件find . -type f -perm grepsedawk2 网络分析工具netstatncnslookup &amp; digtcpdump3 文件传输&amp;下载wgetcurlrsync" }, { "title": "聊聊golang中的slice", "url": "/posts/golang-slice/", "categories": "golang, types & containers, slice", "tags": "golang, slice", "date": "2022-04-12 23:21:33 +0800", "snippet": "本篇文章聊聊 golang 中的核心数据结构 —— slice 的内部实现，从而保证我们能够正确且高效地使用它。源码文件： slice实现：runtime/slice.go append函数: cmd/compile/internal/walk/builtin.go", "content": "本篇文章聊聊 golang 中的核心数据结构 —— slice 的内部实现，从而保证我们能够正确且高效地使用它。源码文件： slice实现：runtime/slice.go append函数: cmd/compile/internal/walk/builtin.go" }, { "title": "聊聊golang中的数据处理", "url": "/posts/golang-buffer/", "categories": "golang, io", "tags": "golang, buffer", "date": "2022-04-12 11:40:48 +0800", "snippet": "本篇文章，我们来聊聊golang中的数据处理。在golang中操作二进制数据，你肯定用过下面的这些类型或者函数： bytes.Buffer io.Copy ioutil.ReadAll这些类或者函数，底层实现是怎样的？在特定的场景下，我们应该如何选择用哪种呢？1 认识golang中的数据读写机制golang中，所有的读写数据的类，都实现了下面的两个基本接口：// 将数据从对象读取到指定...", "content": "本篇文章，我们来聊聊golang中的数据处理。在golang中操作二进制数据，你肯定用过下面的这些类型或者函数： bytes.Buffer io.Copy ioutil.ReadAll这些类或者函数，底层实现是怎样的？在特定的场景下，我们应该如何选择用哪种呢？1 认识golang中的数据读写机制golang中，所有的读写数据的类，都实现了下面的两个基本接口：// 将数据从对象读取到指定slice，必须保证 len(p) 足够容纳想要读取的数据// 若成功，则返回实际读取到的字节数（0 &lt;= n &lt;= len(p))type Reader interface { Read(p []byte) (n int, err error)}// 将指定slice的数据写入到对象// 返回实际写入的字节数（取决于对象内部能容纳多少数据）type Writer interface { Write(p []byte) (n int, err error)}假定，我们现在要写一个Copy(in Reader, out Writer)的函数，实现将数据从 in 拷贝到 out。很简单，只要先调用 A.Read() ，再调用 B.Write() 就完事了。OK，接下来就可以写代码实现了。不过，在此之前，我们要先确认一下 Read 和 Write 方法的具体行为。对于 Read： 当返回的err不为 nil 时，n 可能 &gt; 0，需要注意处理； 当读取到的数据不能填满buf时，返回的 err 可能是 EOF，但也有可能是 nil，取决于具体实现； 当没有数据可读时，所有实现都应当返回 0, EOF这意味着，我们在处理 Read 返回时，应当先处理 n &gt; 0，再处理 err != nil。对于 Write： 当返回的 n &lt; len(p) 时，应当返回一个 非 nil 的 err；接下来，就是我们实现的CopyData了：func CopyData(in Reader, out Writer) (int,error) { // 先创建一个足够容纳要拷贝的数据的slice，这里假定不超过1KB buf := make([]byte, 1024) // 先把数据从 in 拷贝到 buf n, err := in.Read(buf) if n &gt; 0 { // 将buf截断为实际读取到的长度 buf := buf[:n] // 再把数据写到 out n, err = out.Write(buf) return n, err } if err == EOF { return n, nil } return n, err}只要掌握了 Reader 和 Writer 这两个接口的使用，那么其实就已经掌握了 golang 中操作数据的精髓了：数据的处理可以是将一系列 Handler 链在一起。起始处是一个 Reader，中间加工环节都是 Writer + Reader，最后是一个 Writer。2 读取长度未知的数据在上面的示例中，我们第一步中，把数据从 Reader 拷贝到临时 buffer，假定数据不超过某个限制。那么在实际开发中，如果碰到事先不知道数据长度的情况，该怎么办呢？一般地，对于 Reader 接口的实现而言，它的内部一般都会有个指示当前读取到了哪个位置（偏移量）。每次 Read 操作，都会根据实际已读取的字节数，往前移动偏移量。 这里插播另外一个接口：Seeker。对于 Reader，偏移量只能单调地往前进，既不能往后退，也不能往前跳过部分数据，在有些场景可能会带来不便。而 Seeker，顾名思义，实现了 Seek() 方法，可以往前跳跃，也可以往后回退，甚至可以实现倒着读数据。所以，为了读取长度未知的Reader，只要加个循环，不停地以固定size（例如1KB）为单位从Reader拷贝出数据，直到没有数据为止。我们把上面写的单次拷贝的函数实现改名为 CopyBlock，然后再实现下面的CopyData：func CopyData(in Reader, out Writer) (int,error) { bytes := 0 for { n, err := CopyBlock(in[bytes:], out[bytes:]) if n &gt; 0 { bytes += n } if err != nil { return bytes, err } } return bytes, nil}当然，CopyData这么基础的函数，肯定是非常有用的，所以官方提供了一个标准实现：io.Copy()。可以看看它的实现代码：func Copy(dst Writer, src Reader) (written int64, err error) {\treturn copyBuffer(dst, src, nil)}func copyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error) {\t// If the reader has a WriteTo method, use it to do the copy.\t// Avoids an allocation and a copy.\tif wt, ok := src.(WriterTo); ok {\t\treturn wt.WriteTo(dst)\t}\t// Similarly, if the writer has a ReadFrom method, use it to do the copy.\tif rt, ok := dst.(ReaderFrom); ok {\t\treturn rt.ReadFrom(src)\t}\tif buf == nil {\t\tsize := 32 * 1024\t\tif l, ok := src.(*LimitedReader); ok &amp;&amp; int64(size) &gt; l.N {\t\t\tif l.N &lt; 1 {\t\t\t\tsize = 1\t\t\t} else {\t\t\t\tsize = int(l.N)\t\t\t}\t\t}\t\tbuf = make([]byte, size)\t}\tfor {\t\tnr, er := src.Read(buf)\t\tif nr &gt; 0 {\t\t\tnw, ew := dst.Write(buf[0:nr])\t\t\tif nw &lt; 0 || nr &lt; nw {\t\t\t\tnw = 0\t\t\t\tif ew == nil {\t\t\t\t\tew = errInvalidWrite\t\t\t\t}\t\t\t}\t\t\twritten += int64(nw)\t\t\tif ew != nil {\t\t\t\terr = ew\t\t\t\tbreak\t\t\t}\t\t\tif nr != nw {\t\t\t\terr = ErrShortWrite\t\t\t\tbreak\t\t\t}\t\t}\t\tif er != nil {\t\t\tif er != EOF {\t\t\t\terr = er\t\t\t}\t\t\tbreak\t\t}\t}\treturn written, err}可以看到，实现原理跟我们上面实现的CopyData基本原理一样，不过它有个优化点： 如果 Reader 对象同时也实现了 WriterTo 接口，那么直接调用它的 WriteTo 方法； 亦或者，如果 Writer 对象同时也实现了 ReaderFrom 接口，那么直接调用它的 ReadFrom 方法从而无需把数据先拷贝到临时buffer。3 bytes.Buffer聊到现在，我们发现了golang中处理数据的核心，就是 Reader 和 Writer 两个接口了。那么，如果我们现在有一段二进制数据（存放在 []byte 或者 string 中）需要处理，那么我们怎么把它转换成一个 Reader 对象呢？答案就是golang官方库提供的 bytes.Buffer 类型！它同时实现了 Reader 接口和 Writer 接口，并可以用 []byte 或者 string 初始化。bindata := make([]byte, 1024)buf := bytes.NewBuffer(bindata)上例中，我们从 bindata 创建了一个 bytes.Buffer 对象。接着，我们就可以将它当成一个 Reader 来处理 bindata。当作为 Reader 时，bytes.Buffer 内部维护了一个偏移量，以指示当前读取到了哪个位置，这样下次的Read()方法可以接着上次位置继续读取。其实，bytes.Buffer类型的定义也是非常简单的：type Buffer struct {\tbuf []byte // contents are the bytes buf[off : len(buf)]\toff int // read at &amp;buf[off], write at &amp;buf[len(buf)]\tlastRead readOp // last read operation, so that Unread* can work correctly.} lastRead只是为了在调用Unread*系列函数来撤销上次的读取操作时，保存上次的读操作类型。所谓“撤销读”操作，其实就是将偏移量回退到上次读操作之前的位置。那么，当然要知道上次是读取了什么类型的数据了（即上次读取了几个字节）。当作为 Writer 时，bytes.Buffer内部会自动分配足够的内存以保存写入的数据，而无需使用者关心。// 当作为Writer使用时，创建一个零值的buffer即可buf := bytes.Buffer{}data := make([]byte, 1024)// 当写入数据时，buffer内部自动扩容buf.Write(data)要想了解 Buffer 的内部扩容机制，最好先了解下 slice 内部的内存分配机制，然后再结合 Buffer 的Write()方法的实现代码一起理解即可。4 聊聊链式处理的劣势不知道大家有没有注意到，Reader + Writer 的方式，只能形成简单的单链处理逻辑。因为一个 Reader，你一旦把它的数据读取了，那么就没法再读取第二遍。要想在某个节点处分叉，那么必须先把数据拷贝出来([]byte) ，然后再基于该 slice,重新初始化多个新的 Reader 去处理它。所以这里又有了个新需求：将数据从 Reader 一次性全部拷贝到单个 slice（以下称其为 resultSlice）。这就是官方库里的 io.ReadAll()。// 从 Reader 一直读数据，直到出错或者返回EOF，并返回成功读取到的数据bytes, err := io.ReadAll(in)因为无法事先知道 Reader 的数据长度，所以 ReadAll的内部实现，一种可能的方案，就是在一个循环内，不停地按固定size读取出数据（unitSlice），再将 unitSlice 数据 append 到 resultSlice，直到EOF为止。这里贴下它的实现：func ReadAll(r Reader) ([]byte, error) {\tb := make([]byte, 0, 512)\tfor {\t\tif len(b) == cap(b) {\t\t\t// Add more capacity (let append pick how much).\t\t\tb = append(b, 0)[:len(b)]\t\t}\t\tn, err := r.Read(b[len(b):cap(b)])\t\tb = b[:len(b)+n]\t\tif err != nil {\t\t\tif err == EOF {\t\t\t\terr = nil\t\t\t}\t\t\treturn b, err\t\t}\t}}这里的实现比较不直白。它并不是以固定的size去循环读，而是先分配个初始的 512Bytes的 slice，然后有额外的数据时，先依赖 slice本身的内存扩容机制（每扩容一次，需要将数据全部从旧位置复制到新位置）将其扩容，再将数据直接写到扩容后的空间。数据的序列化&amp;反序列化上面我们说了很多，但是都只是涉及到数据的复制，没有涉及到对数据的处理。实际开发中，我们经常会碰到数据的序列化和反序列化问题。 所谓“序列化”，就是将一个具有某种结构（类型）的对象，按照某种规则，转换成一段二进制数据（一般是[]byte）； 所谓“反序列化”， 就是从一段二进制数据，按照某种规则，将其转换回特定结构（类型）的对象。反序列化对于反序列化，可以将数据的来源抽象成 io.Reader，然后一边读，一边进行数据转换。打个比方，我们在做服务开发时，肯定会涉及到 API 的调用（HTTP RESTFul API，或者 grpc/brpc等RPC框架）。这里以 HTTP 请求返回 JSON 格式的数据为例：type User struct {\tId string `json:\"id\"`\tName string `json:\"name\"`\tAddress string `json:\"address\"`\tTel string `json:\"tel\"`}func GetUserInfo(id string) (*User, error) {\tresp, err := http.Get(\"https://example.com/users/\" + id)\tif err != nil {\t\treturn nil, err\t}\tdefer resp.Body.Close()\tuser := User{}}这里，我们需要实现一个 GetUserInfo 函数，从服务端查询回JSON格式的数据，然后将其转换成一个User类型的对象。请求返回的 Body 数据存储在一个 resp.Body 对象中，它的接口类型是io.ReadCloser（ io.Reader + io.Closer）。一种比较直观的方法就是，我们先把它的数据全部读取出来（io.ReadAll），然后调用 json.Unmarshal 完成 json 反序列化。\tdata, err := io.ReadAll(resp.Body)\terr = json.Unmarshal(data, &amp;user)\tif err != nil {\t\treturn nil, err\t}\treturn &amp;user, nil}当然，熟悉使用json库的同学，肯定知道还有另外一种方法完成反序列化：decoder := json.NewDecoder(resp.Body)decoder.Decode(&amp;user)这里引入了一种新的类型：json.Decoder。创建它时，需要传入一个 Reader 对象；然后在调用Decode()时，直接从 Reader 对象一边读，一边反序列化（就跟从 []byte 边读边序列化一样）。相比于上面的方法，这种方法减少了一次数据复制的开销，性能更强。序列化对于序列化，将数据的去处，抽象化为 io.Writer 对象，然后一边转换，一边将数据写到 Writer 即可。还是以上面的场景为例，这次我们需要实现一个 UpdateUserInfo() 函数，去服务端更新用户信息。func UpdateUserInfo(user *User) error {\tbuf := bytes.Buffer{}\tencoder := json.NewEncoder(&amp;buf)\terr := encoder.Encode(user)\tif err != nil {\t\treturn err\t}\tresp, err := http.Post(\"https://example.com/users\", \"application/json\", &amp;buf)\tif err != nil {\t\treturn err\t}\tdefer resp.Body.Close()\tif resp.StatusCode != http.StatusOK {\t\treturn fmt.Errorf(\"PostFailed: %d(%s)\", resp.StatusCode, resp.Status)\t}\treturn nil}思考下这里数据的走向，涉及到两个环节：序列化（结构体对象 -&gt; 二进制数据）和 HTTP发包（二进制数据 -&gt; 网络协议栈）。所以，这里我们使用 bytes.Buffer 来存储二进制数据。在序列化环节，它是一个 Writer；在发包环节，它是一个 Reader。总结本篇文章，从最基础的 Reader 接口 和 Writer 接口入手，一步步地为大家介绍了 io.Copy，bytes.Buffer 和 io.ReadAll 的使用和内部实现。尤其需要注意的是，对于数据处理，优先考虑是否将其抽象化为 Reader 和 Writer。只有在特别必要的场景下，才需要使用io.ReadAll将原始数据拷贝出来。" }, { "title": "Kafka原理学习之协议交互流程", "url": "/posts/kafka-protocol/", "categories": "kafka", "tags": "kafka", "date": "2021-02-04 17:00:00 +0800", "snippet": "要想理解某个系统是怎么运行的，首先我们可以看看它提供什么样的API。本文从 Kafka 的协议交互流程入手，分析 Producer 和 Consumer 是如何工作的。一方面，可以用来实现自己的 kafkasdk；另一方面也能更好地理解 Kafka 的内部原理。接下来就从以下3个方面来学习Kafka协议： Kafka协议格式，包括编解码方案； Producer 工作流程； Consum...", "content": "要想理解某个系统是怎么运行的，首先我们可以看看它提供什么样的API。本文从 Kafka 的协议交互流程入手，分析 Producer 和 Consumer 是如何工作的。一方面，可以用来实现自己的 kafkasdk；另一方面也能更好地理解 Kafka 的内部原理。接下来就从以下3个方面来学习Kafka协议： Kafka协议格式，包括编解码方案； Producer 工作流程； Consumer 工作流程 本文基于 Kafka 1.0 版本描述，较新版本(v2以上)肯定有出入，但核心逻辑没有改变1 Kafka协议格式这里主要参考 Kafka 官方提供的 KAFKA PROTOCOL GUIDE。如果你要自己实现 kafka Client，那么建议最好把它打印出来放在手边，一个字一个字地看 n 遍。如果你只是想要了解 Producer 或者 Consumer 的工作流程，那么只需要看看我接下来总结的内容即可。Kafka协议可以分为 Request 和 Response。从某种程度来说，Kafka更多的是提供了 RPC 功能：请求只能由 Client 主动发到 Broker；Broker 针对每个请求回复一个响应给 Client。不同的 Request 使用不同的 apikey 来区分；Request 和 Response 通过 CorrleationId 来一一对应。编解码方案从编解码角度来说，每个协议包都是由 4字节的 size 开头，后面再跟相应字节的请求包或响应包。在1.0及以前版本中，Kafka协议中可使用的数据类型仅有3种： 固定长度的整形，包括 int8, int16, int32, int64 等； 可变长度的字符串，包括 string, bytes 等； 复合类型，包括 array 等。每种类型都有特定的编解码方案，具体可以参考官方文档，这里不再详述。从2.0版本开始，又增加了很多复杂的类型，比如 boolean, varint, varlong, uuid, float64, compact_string, compact_bytes 等等。Request &amp; ResponseKafka最新版本中提供的Request已经达到50多种了，但是比较核心的其实也就下面几种： 请求 说明 Metadata 查询集群当前Broker列表，以及指定的topic信息，包括partition数量以及leader/replicas信息 Produce 发布消息 Fetch 从指定偏移量开始拉取某个（些）partition的消息 Offset 查询某个（些）partition的offset信息，可以指定时间戳 Offset Commit 提交offset，只针对ConsumerGroup Offset Fetch 查询某个ConsumerGroup当前提交的offset信息 此外，从0.9版本开始，Kafka提供了消费组的概念，并相应地提供了一组管理协议，包括 GroupCoordinator/JoinGroup/SyncGroup/LeaveGroup/Heartbeat 等，具体在后面的Consumer流程中再讲。Request每个请求都有固定的 header，具体格式如下：struct RequestHeader { int32_t size; // 请求总长度 int16_t apikey; // 区分请求类型 int16_t version; // 区分请求版本 int32_t correlationId; // 请求上下文，用于对应回包 std::string clientid; // 请求方标识，仅用来打日志};在具体发包时，header 在前（这不废话吗！），后面再跟具体的请求包。请求包的大小等于总的 size 减去 header 的大小。版本兼容注意到头部的 version 字段，它是用来保证客户端和服务端版本兼容的。Kafka保证的兼容策略是 bidirectional compatibility。即，新版本客户端可以访问旧版本的 broker；新版本 broker 可以接受旧版本的客户端的请求。客户端在连接上 Broker 并实际开始工作之前，可以先发送 ApiVersionsRequest 请求到每个 Broker，以查询 Broker 支持的 版本列表，并从中选取一个它能识别的最高版本作为后续使用版本。并且这个版本协商是基于连接的：每次连接断开并重连时，都要重新进行版本协商。因为断线可能正是因为Broker升级导致的。Response每个回包也有固定的 header，具体格式如下：struct ResponseHeader{ int32_t size; int32_t correlationId;};回包的 header 就很简单了，只有一个 correlationId。所以客户端必须要把处理回包时要用到的信息全部在发出请求时保存在请求上下文中，然后通过 correlationId 找到上下文。C++实现根据官方文档中的编解码规范，我们就可以自己写一个C++版本的编解码实现了。kafkaprotocpp的关键类有： Pack &amp; Unpack。负责各种Kafka支持的数据类型的编解码； Request。负责 Request 的编解码； Response。负责 Response 的编解码； Marshallable。所有具体的请求或响应，都需要继承此抽象基类，实现自己的 marshal/unmarshal方法。以MetadataRequest为例，请求协议定义如下：struct MetadataRequest : public Marshallable{ enum { apikey = ApiConstants::METADATA_REQUEST_KEY, apiver = ApiConstants::API_VERSION0}; std::vector&lt;std::string&gt; vecTopic; virtual void marshal(Pack &amp;pk) const { pk &lt;&lt; vecTopic; } virtual void unmarshal(const Unpack &amp;up) { up &gt;&gt; vecTopic; }};struct MetadataResponse : public Marshallable{ std::vector&lt;Broker&gt; vecBroker; std::vector&lt;TopicMetadata&gt; vecTopicMeta; virtual void marshal(Pack &amp;pk) const { pk &lt;&lt; vecBroker &lt;&lt; vecTopicMeta; } virtual void unmarshal(const Unpack &amp;up) { up &gt;&gt; vecBroker &gt;&gt; vecTopicMeta; }};收发请求如下：std::string topic = \"test\";MetadataRequest metareq;metareq.vecTopic.push_back(topic);Request req(1, \"meta_test\", metareq.apikey, metareq.apiver, metareq);// 发包到指定套接口int ret = send_request(sockfd, req.data(), req.size());// 收包char* buf = read_response(sockfd);size_t len = Response::peeklen(buf);Response resp(buf, len);resp.head();MetadataResponse meta;try {\tmeta.unmarshall(resp.up);} catch(PacketError&amp; pe) {}// 处理Meta信息感兴趣的可以具体去我的github看它的源码。2 Partition存储模型在深入了解 Producer 和 Consumer 的交互流程之前，我们先来看下Kafka的存储模型。在 Kafka 中，topic 是消息的逻辑单元：不同的 topic 代表了不同的业务数据，是完全相互独立的。partition 则是消息存储的物理单元：每个 topic 可以分成若干个 partition，不同的 partition 可以存储在不同的 Broker 上。我们先来看下如何在 Kafka 中创建 topic：$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 3 --topic test这里我们指定了3个参数： --topic表示名称，必须唯一； --partitions表示分区个数，据消息并发吞吐量和客户端处理能力设置； --replication-factor表示消息备份数，决定了数据的可靠性接下来可以看到这个topic的具体分区信息：$ kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic testTopic:test PartitionCount:3\tReplicationFactor:2\tConfigs:\tTopic: test Partition: 0\tLeader: 402\tReplicas: 402,401\tIsr: 402,401\tTopic: test\tPartition: 1\tLeader: 403\tReplicas: 403,402\tIsr: 403,402\tTopic: test\tPartition: 2\tLeader: 401\tReplicas: 401,403\tIsr: 401,403当我们在创建topic的时候，kafka会创建指定数量的partition，并将其存储在若干个（由ReplicationFactor决定）Broker上。在这些Broker中，会选出一个作为这个 partition 的 leader，来负责它的生产和消费请求。 在Kafka集群中，会有一个Broker被选举为集群的 Controller （借助Zookeeper），来负责partition的分配工作，重点是保证集群内各Broker的负载均衡。例如，这里 Partition 0 落在了 402,401 这两个Broker上，并且其中排在前面的那个就是它的 Leader。当我们要发布或消费这个 partition 的消息时，必须将 Producer &amp; Fetch请求发到这台 Broker。3 Producer 工作流程接下来看下 Producer 发布消息到 Kafka 时的流程，看看中间都经历了些什么：这里，我们假定集群有2台Broker，每台Broker各自是一个partition的leader。那么Producer的具体流程可以描述为： Producer向 BootstrapServer 发送 MetadataRequest，查询集群当前Broker列表，以及partition的leader信息； Producer向 Broker1 请求发布消息到 partition-0，收到成功回复； Kafka集群发生 leader 转移，partition-0 的 leader 变成了 Broker2； Producer再次向 Broker1 发布消息，收到错误码（UNKNOWN_TOPIC_OR_PARTITION）； Producer再次向任意一台 Broker 发送 MetadataRequest，查询最新的 leader 信息； Producer重新向 Broker2 请求发布消息到 partition-0，收到成功回复。可以看到，整个发布流程只涉及到两个协议：MetadataRequest 和 ProduceRequest。MetadataRequest官方格式定义：Metadata Request (Version: 0) =&gt; [topics] topics =&gt; name\tname =&gt; STRINGMetadata Response (Version: 0) =&gt; [brokers] [topics] brokers =&gt; node_id host port node_id =&gt; INT32 host =&gt; STRING port =&gt; INT32 topics =&gt; error_code name [partitions] error_code =&gt; INT16 name =&gt; STRING partitions =&gt; error_code partition_index leader_id [replica_nodes] [isr_nodes] error_code =&gt; INT16 partition_index =&gt; INT32 leader_id =&gt; INT32 replica_nodes =&gt; INT32 isr_nodes =&gt; INT32Metadata主要查询两种信息： 集群的Broker列表，包括 node_id, host, port 等信息； 指定的topic信息，包括 partition 数量，以及每个 partition 的 leader, replicas, isrs等。如果查询时没有指定任何 topic，那么会查询到集群所有的 topic 信息。Kafka Client 有两种场景下会发送MetadataRequest： 启动之后定期查询，以便感知到新的Broker信息和topic信息（例如partition扩容了）； 当生产或消费时，提示当前Broker已经不是Leader了，需要及时更新信息正是因为有Metadata协议的存在，Kafka Client在运行过程中总是能动态感知到集群所有的Broker信息。因此，我们在启动 Producer 或 Consumer 时，配置的bootstrap.server只需要包含一台可用的Broker信息就可以了。此外，我们通过MetadataResponse获取到的 Broker 的 host 就是我们在部署 Kafka 时配置的advertised.listeners项。需要注意它和listeners的区别： listeners配置的是 Kafka 监听的套接口，例如我们可以配置为PLAINTEXT://0.0.0.0:9092来监听本机所有网口； advertised.listeners是写入到ZooKeeper进而被客户端通过MetadataRequest拿到的地址举个例子，假定我们的Kafka部署在双线或多线机房，为了保证高可用，我们通常是配置为监听所有网口。但是advertised.listeners又不能配置为0，所以我们可以给它配置成一个域名：这个域名再绑定Broker的所有网口的具体IP。这样KafkaClient拿到域名后就可以解析到多个IP，并在连接断开时可以尝试使用另外的IP来重连。ProduceRequest官方协议定义：v0, v1 (supported in 0.9.0 or later) and v2 (supported in 0.10.0 or later)ProduceRequest =&gt; RequiredAcks Timeout [TopicName [Partition MessageSetSize MessageSet]] RequiredAcks =&gt; int16 Timeout =&gt; int32 Partition =&gt; int32 MessageSetSize =&gt; int32v2 (supported in 0.10.0 or later)ProduceResponse =&gt; [TopicName [Partition ErrorCode Offset Timestamp]] ThrottleTime TopicName =&gt; string Partition =&gt; int32 ErrorCode =&gt; int16 Offset =&gt; int64 Timestamp =&gt; int64 ThrottleTime =&gt; int32从协议可以看出来，这是一个批量接口：每次请求可以指定发数据给多个 topic 或者多个 partition 。这里重点看一下 RequireAcks 参数，它决定了生产者往Kafka发消息的可靠性。它表示的意思是：Leader 收到请求后需要等待多少个 replicas 的 ack，才能回包给客户端。它的取值为： 0，表示Leader不会发送回包给客户端； 大于0且小于 ReplicaFactor 的整数，表示将数据同步到指定数量的 Broker 上才给客户端回包； -1，表示Leader会将数据同步到所有 ISR 集合中的Broker后才给客户端回包。所以，RequireAcks值越大，表示可靠性越高，但是效率就相应地越低了。不过要更详细地描述可靠性的话，还得理解一个概念 ISR。在Kafka内部，Leader 和 Follower 间的数据同步，依靠每个 Follower 通过 long-pull 模式一直不停地从 Leader 拉取数据。那自然地，Leader 会维护每个 Follower 拉取到了哪个 offset，以及与最新offset的差值是多少。当这个差值（lag值）不超过某个既定值时，就认为这个 Follower 是跟 Leader 保持同步的，属于 In-Sync-Replicas 集合。显然，这个 ISR 集合是动态变化的。当某个 Follower 长时间没有过来拉，或者 lag 值比较大时，就会被踢出 ISR；当它恢复之后，又会被重新加入 ISR 集合。只有被 ISR 集合中所有的 Broker 都同步的消息，才被认为是已提交的（committed）。只有已提交的消息，才能被消费者看到。如果当前 leader 挂了，因为已提交的消息肯定在 ISR 集合中的其它 Broker 上都存在，所以只要ISR集合不为空，那么重新选一个作为 Leader 即可。但是如果 ISR 为空呢？这取决于另外一个参数 unclean.leader.election.enable：如果设置为 true，那么可以选择一个不在 ISR 集合中的 Replica 作为 leader。但是这可能导致部分已提交的消息丢了，相当于是拿 可靠性 换 可用性。 此参数可全局设置，也可针对 topic 设置。再说回 ProduceRequet。假定我们有个 partition 的 ReplicaFactor 为3，表示会存储在 3 台 Broker（包括Leader）上。如果发布时的 RequireAcks 填了 3，那就表示每次发布都要 3 台都同步到数据才算成功。那如果 ISR 集合中没有 3 台会怎样呢？答案就是不可写。从某种程度来说，我们在创建 topic 时指定的 ReplicaFactor 就已经表示了我们对这个 topic 的数据可靠性的要求了。那如果在发布时再去设置ack值，感觉有点冗余了。而且有时候我们在发布的时候，不太好知道这个topic的具体ReplicaFactor值。所以，我们可以将 acks 填为 -1，表示等待当前ISR集合中都同步了就算成功。在一切正常的情况下，ISR 集合就等于 Replicas 集合；当出问题时，有问题的Broker就会被踢出 ISR 集合。考虑到在不出问题的时候，除Leader之外的Replicas是发挥不出作用的，所以如果没有其它机制保障的话，acks 填 -1，好像可靠性不太“稳定”。所以 Kafka 提供了另外一个参数min.insync.replicas。当 acks 填 -1 时，如果 ISR 集合数量小于此值的话，拒绝写入数据。这样就给可靠性设置了一个底线。 此参数可全局设置，也可针对 topic 设置。4 Consumer 工作流程消费组的工作原理，其实可以分解成3个相互独立的子过程： 组关系的维护。包括 JoinGroup, SyncGroup, LeaveGroup 以及维持组状态的心跳包 HeartBeat； offset偏移量的管理。包括 FetchOffset, FetchCgroupOffset, CommitOffset 等； 拉取消息。包括 FetchMessage 等。这3个过程相互独立，从协议交互角度来说，你可以单独调用每个过程涉及到的协议来实现特定目的。维护消费组关系关于Coordinator为了保证数据的一致性，每个消费组的状态都由某个固定的 Broker 来管理。这个 Broker 称为该消费组的 Coordinator。从负载均衡角度来讲，集群内每个Broker都是差不多数量的消费组的 Coordinator。针对特定消费组来说，它的所有的组管理相关的请求都必须发送给 Coordinator 才能被处理。因此，Consumer 启动后的第一件事，就是查询它的 Coordinator。方法很简单，发送 QueryCoordinator 请求到任意一台 Broker 即可。关于消费组在一个消费组里，每个 Consumer 都会被 Coordinator 分配一个唯一的 memberid。并且，Coordinator会挑选一个 Consumer 作为这个消费组的 leader。所谓消费组，就是多个消费者共同出力来消费某个（些）topic的所有parittion。所有的这些 partition 会被均衡地分配给所有的消费者。也因此，partition 数量一般不会超过消费者的数量。当然，如果只有1个parittion，为了保证高可用，也会起2个消费者，以便当其中1个出问题的时候，另1个能立即接管过来。那谁来负责在组内分配 partition 呢？你可能会觉得是 Coordinator，但其实不是！真正负责分配的是消费组的 leader Consumer。这也是 Coordinator 的名字的由来：Broker 只是帮忙协调和维护组关系，具体涉及到消费的活（包括分配），都是由 Consumer 自己完成。关于消费组状态维护组关系，其本质就是在客户端中维护一个消费组的状态机，如下图所示：在描述状态机转移过程之前，我们先来看一下一个正常的消费组的状态：每个成员都处于 CS_UP 状态，各自消费自己负责的 partition，并且需要每隔固定间隔（不超过配置值 heartbeat.interval.ms）发送心跳包给 Coordinator 来维持状态，否则就会被踢出消费组。好了，再来看 Consumer 的状态机转移过程： 消费者启动后默认是CS_DOWN状态，然后发送JoinGroup给 Coordinator 来请求加入组，并转变为CS_JOINING状态； 当 Coordinator 察觉到成员有变动时，它会在每个现有成员的下一个心跳包回包中告知它们，需要重新发起JoinGroup。这样，每个现有成员就从 CS_UP状态变为CS_JOINING； 当 Coordinator 收到所有成员的 Join 请求后，就会从中选出新的 leader，并且通过 JoinResponse 告知所有成员谁是 leader。其中，给 leader 的回包中，会带上所有成员的信息以及它们订阅的topic列表； 当消费者收到 JoinResponse 后，就会知道自己是不是 leader。如果是 leader，就需要分配 partition 了，然后把分配结果放在 SyncGroup 中发送给 Coordinator。除了 leader 之外，其它成员也都需要发送 SyncGroup，只不过不用带分配结果。这样，所有成员都从 CS_JOINING 变成了 CS_SYNCING； Coordinator 收到所有成员的 SyncGroup 请求后，会将 leader 上传的分配结果放到 SyncResponse 告知给所有成员。这样所有成员收到回包后，就知道自己该负责消费哪些 partition 了，然后状态从 CS_SYNCING 变成 CS_UP，就可以准备干活了。 此后，每个成员要继续定期发送 Heartbeat 以保持在 CS_UP 状态。以上就是整个消费组的状态转移过程了。为了避免一些非法的消费者进程（例如那些卡了很久然后突然又恢复了）干扰消费组状态，Coordinator 会为每个消费组维护一个单调递增的 generationId。每次有成员变动时，generationId都会加1。当收到generationId与当前值不一致的请求时，会拒绝。关于消费组偏移量当拿到分配结果后，Consumer 就准备开始干活了。相比于单机版的消费模式，消费组除了能负载均衡之外，还有另外一个好处：Coordinator 可以帮助存储上次消费到的偏移量，以便当某个partition从一个消费者转移到另一个消费者时，可以接着消费，从而保证消息不丢失。所以，Consumer 在具体拉取消息之前，要首先能够知道该从哪个位置开始拉取。方法也很简单，发送 OffsetFetchRequest 到 Coordinator 就可以获取到之前提交的便宜量了。v0 and v1 (supported in 0.8.2 or after):OffsetFetchRequest =&gt; ConsumerGroup [TopicName [Partition]] ConsumerGroup =&gt; string TopicName =&gt; string Partition =&gt; int32v0 and v1 (supported in 0.8.2 or after):OffsetFetchResponse =&gt; [TopicName [Partition Offset Metadata ErrorCode]] TopicName =&gt; string Partition =&gt; int32 Offset =&gt; int64 Metadata =&gt; string ErrorCode =&gt; int16那如果这个消费组之前没人提交过对应 partition 的 offset 呢？那就需要用另外一个协议了——发送 OffsetRequest 到 partition 的 Leader Broker。// v0ListOffsetRequest =&gt; ReplicaId [TopicName [Partition Time MaxNumberOfOffsets]] ReplicaId =&gt; int32 TopicName =&gt; string Partition =&gt; int32 Time =&gt; int64 MaxNumberOfOffsets =&gt; int32 // v1 (supported in 0.10.1.0 and later)ListOffsetRequest =&gt; ReplicaId [TopicName [Partition Time]] ReplicaId =&gt; int32 TopicName =&gt; string Partition =&gt; int32 Time =&gt; int64这个请求可以查询到指定 partitions 的偏移量信息。其中，参数 Time 可以指定要查询哪个时间戳的便宜量，取值可以为： &gt; 0。表示查询指定时间戳（单位ms）之前的最后一条消息的offset； -1。表示查询最近的offset（latest）； -2。表示查询最老的offset（earlist）。这也是我们在配置 Consumer 时经常碰到的一个参数：auto.offset.reset。只不过比较奇怪的是，明明协议层面可以支持配置一个具体的时间戳，但是所有的Client暴露出来的接口，只能配置成 earliest 或 latest。当消费组正常消费时，可以随时把已经消费过的偏移量提交到 Coordinator。方法就是发送 OffsetCommitRequest 到 Coordinator。提交到 Coordinator 的offset信息也是有个有效期的，当超过规定时候没有提交时，Broker 也会把 offset 给删掉的。这样也会重新触发上面提到的没有初始偏移量的逻辑。这里有个点，就是 Coordinator 不会去校验你提交的offset是否合法。换句话说，它只是提供了一个 key 为 'groupid/topic/partition'，value 为 int64 的读写接口。 我们可以利用这个特点，来为Kafka跨集群做数据同步。Kafka跨集群同步，方法一般就是在源机房部署一套消费者，然后将消息发布到目的机房。 那这里就涉及到是采用 同机房消费，跨机房发布 还是 跨机房消费，同机房发布 的选择了。 跨机房消费，意味着消费组的状态不稳定，频繁的网络超时会导致消费组的 rebalance。在消费组 rebalance 时，所有消费都需要暂停。但是跨机房拉取消息本身没有副作用； 跨机房发布，意味着当网络超时时，发布端需要重发，导致目的机房消息重复。(不过新版本Kafka已经支持发布端逻辑去重了) 不过借助上面提到的特点，我们可以在目的机房部署消费组，但是把消费组的组管理和offset管理放在目的机房Kafka（即在目的机房加入消费组），但是提交的offset其实是源机房partition的offset， 消息还是从源机房拉取。 这样就综合了两种方案的优点，并且规避了各自的缺点。不过这种方案需要定制化Kafka的SDK。关于拉取消息有了 partition 列表，有了每个 partition 的初始offset，那么接下来 Consumer 的工作就很简单了，只要通过 long-pull 模式不停地去各自 partition 的leader 拉取消息即可。拉取消息通过发送 FetchRequest 给 leader：FetchRequest =&gt; ReplicaId MaxWaitTime MinBytes [TopicName [Partition FetchOffset MaxBytes]] ReplicaId =&gt; int32 MaxWaitTime =&gt; int32 MinBytes =&gt; int32 TopicName =&gt; string Partition =&gt; int32 FetchOffset =&gt; int64 MaxBytes =&gt; int32v1 (supported in 0.9.0 or later) and v2 (supported in 0.10.0 or later)FetchResponse =&gt; ThrottleTime [TopicName [Partition ErrorCode HighwaterMarkOffset MessageSetSize MessageSet]] ThrottleTime =&gt; int32 TopicName =&gt; string Partition =&gt; int32 ErrorCode =&gt; int16 HighwaterMarkOffset =&gt; int64 MessageSetSize =&gt; int32在拉取时，可以指定 MinBytes 和 MaxBytes，来指定本次拉取最少和最多拉取多少数据，以及最多等待时间 MaxWaitTime。在回包中，Kafka除了返回具体的消息之外，还会返回一个参数HighwaterMarkOffset，表示该 partition 目前可供消费者消费的最新的offset。通过此值我们可以知道还有多少消息待拉取。这里需要注意的是，HighwaterMarkOffset表示的是消费者能看到的最新offset，不表示发布者最新发布的offset。这个涉及到Kafka内部同步机制，只有被所有 ISR 集合中的Broker同步的消息，才能增加 HighwaterMarkOffset。5 总结除了 Producer 和 Consumer 相关协议之外，Kafka还提供一些管理类的API，包括 ListGroup（列出所有消费组）、DescribeGroups（查询消费组状态）等等。新版kafka还提供了创建Topic之类的APi。深度利用这些协议可以用来写一些Kafka的监控管理插件。弄懂Kafka的协议交互流程，除了可以加深对Kafka的理解之外，还有以下的好处： 帮助定位生产者和消费者的问题。由于Kafka Broker不会打印任何与客户端相关的异常日志信息，全部都是以错误码的形式告知给客户端。因此了解协议交互流程，就可以更好地从客户端侧的日志了解到具体是哪个环节出问题； 实现自己的KafkaSdk。由于作者在工作中需要实现Kafka的跨集群同步数据，而开源的sdk都不太适合，因此只能自己实现了一个C++版sdk； 兼容Kafka协议来提供服务。Kafka已经在互联网行业得到大规模应用，如果新开发的MQ系统可以兼容Kafka协议，那么可以掠夺大量的Kafka使用者，实现无缝迁移。例如最近比较火热的 pulsar，就是通过 Kop 插件来兼容了Kafka协议。希望本篇文章可以让大家更加理解Kafka的工作模式。" } ]
